# âš½ Ligue 1 Analytics - Plateforme de Scraping et Visualisation

## ğŸ“‹ Description du Projet

Plateforme complÃ¨te d'analyse et de visualisation des donnÃ©es du championnat de France de football (Ligue 1). Le projet combine un systÃ¨me de scraping automatisÃ© avec un dashboard interactif pour suivre en temps rÃ©el le classement et les statistiques des Ã©quipes.

### FonctionnalitÃ©s principales

- ğŸ•·ï¸ **Scraping automatisÃ©** : Collecte pÃ©riodique des donnÃ©es depuis Wikipedia
- ğŸ“Š **Dashboard interactif** : Visualisation en temps rÃ©el via Dash/Plotly
- ğŸ’¾ **Stockage MongoDB** : Base de donnÃ©es NoSQL pour la persistance
- ğŸ³ **Architecture containerisÃ©e** : DÃ©ploiement facile avec Docker Compose
- ğŸ”„ **Mise Ã  jour automatique** : Scheduler configurable pour les scraping pÃ©riodiques
- ğŸ“ˆ **Analytics avancÃ©s** : Graphiques, tableaux et statistiques dÃ©taillÃ©es

---

## ğŸ—ï¸ Architecture Technique

### Stack Technologique

| Composant | Technologie | Version | RÃ´le |
|-----------|------------|---------|------|
| **Web Scraping** | Scrapy | 2.11.0 | Framework de scraping |
| **Scheduler** | Schedule | 1.2.0 | Planification des tÃ¢ches |
| **Base de donnÃ©es** | MongoDB | 7.0 | Stockage NoSQL |
| **Backend** | Python | 3.x | Logique mÃ©tier |
| **Frontend** | Dash | 2.14.2 | Framework web interactif |
| **Visualisation** | Plotly | 5.18.0 | Graphiques interactifs |
| **Containerisation** | Docker | - | Orchestration des services |
| **ODM** | PyMongo | 4.6.1 | Driver MongoDB |

### Architecture des Services

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   Docker Compose                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚   Spider        â”‚    MongoDB      â”‚    Dashboard        â”‚
â”‚   Container     â”‚    Container    â”‚    Container        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ - Scrapy        â”‚ - Mongo 7.0     â”‚ - Dash/Plotly       â”‚
â”‚ - Scheduler     â”‚ - Port: 27017   â”‚ - Port: 8050        â”‚
â”‚ - Python        â”‚ - Volumes       â”‚ - MongoDB Client    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚                  â”‚                  â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                   ligue1-network
```

### Structure du Projet

```
DataEngineering/
â”œâ”€â”€ docker-compose.yml          # Orchestration des services
â”œâ”€â”€ health_check.py             # Script de vÃ©rification des services
â”‚
â”œâ”€â”€ scraper/                    # Service de scraping
â”‚   â”œâ”€â”€ Dockerfile              # Image Docker du spider
â”‚   â”œâ”€â”€ requirements.txt        # DÃ©pendances Python
â”‚   â”œâ”€â”€ scheduler.py            # Scheduler de scraping
â”‚   â”œâ”€â”€ scrapy.cfg              # Configuration Scrapy
â”‚   â””â”€â”€ ligue1_scraper/         # Projet Scrapy
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ items.py            # DÃ©finition des items
â”‚       â”œâ”€â”€ pipelines.py        # Pipeline MongoDB
â”‚       â”œâ”€â”€ settings.py         # Configuration Scrapy
â”‚       â””â”€â”€ spiders/
            â””â”€â”€ __init__.py
â”‚           â””â”€â”€ ligue1_spider.py  # Spider principal
â”‚
â””â”€â”€ webapp/                     # Service dashboard
    â”œâ”€â”€ Dockerfile              # Image Docker du dashboard
    â”œâ”€â”€ requirements.txt        # DÃ©pendances Python
    â”œâ”€â”€ app.py                  # Application Dash
    â””â”€â”€ mongo_client.py         # Client MongoDB
```

---

## ğŸš€ Installation et Lancement

### PrÃ©requis

- **Docker** >= 20.10
- **Docker Compose** >= 2.0
- **Git** (pour cloner le projet)
- **Ports disponibles** : 27017 (MongoDB), 8050 (Dashboard)

### 1. Cloner le Projet

```bash
git clone https://github.com/Sachachen/DataEngineering.git
cd DataEngineering
```

### 2. Lancer l'Application

```bash
# Une seule commande et tout dÃ©marre !
docker-compose up -d
```

#### MÃ©thode dÃ©taillÃ©e (Ã©tape par Ã©tape)

```bash
# D'abord on construit les images Docker
docker-compose build

# Ensuite on lance tout en arriÃ¨re-plan
docker-compose up -d

# Et si on est curieux, on regarde les logs dÃ©filer
docker-compose logs -f
```

### 3. VÃ©rification du DÃ©ploiement

```bash
# VÃ©rifier l'Ã©tat des conteneurs
docker-compose ps

# VÃ©rifier les logs de chaque service
docker-compose logs mongodb
docker-compose logs spider
docker-compose logs webapp

# Lancer le health check
python health_check.py
```

### 4. Admirer votre Å“uvre ğŸ¨

Ouvrez votre navigateur prÃ©fÃ©rÃ© et allez sur : **http://localhost:8050**

Et voilÃ  ! Vous devriez voir un magnifique dashboard avec toutes les stats de la Ligue 1. Si Ã§a marche du premier coup, vous pouvez vous taper dans le dos ! ğŸ‘

---

## ğŸ”§ Configuration

### Variables d'Environnement

#### MongoDB
- `MONGO_INITDB_ROOT_USERNAME` : admin
- `MONGO_INITDB_ROOT_PASSWORD` : password123
- `MONGO_INITDB_DATABASE` : ligue1_db

#### Spider
- `MONGO_URI` : URI de connexion MongoDB
- `LOG_LEVEL` : Niveau de logs (INFO, DEBUG, WARNING)

#### Webapp
- `MONGO_URI` : URI de connexion MongoDB

### Configuration du Scheduler

Le spider peut Ãªtre configurÃ© avec diffÃ©rents paramÃ¨tres :

```bash
# Lancer immÃ©diatement puis toutes les heures
docker-compose up -d  # Par dÃ©faut : --immediate --interval 1

# Modifier l'intervalle de scraping
# Editer docker-compose.yml ligne :
command: python scheduler.py --immediate --interval 2  # Toutes les 2 heures
```

Options du scheduler :
- `--immediate` : Lance un scraping au dÃ©marrage
- `--interval N` : Interval en heures entre chaque scraping (dÃ©faut: 1)

---

## ğŸ“Š Fonctionnement DÃ©taillÃ©

### 1. Service de Scraping (Spider)

#### Source de DonnÃ©es
- URL : `https://fr.wikipedia.org/wiki/Championnat_de_France_de_football_2025-2026`
- Parser : HTML via Scrapy CSS Selectors

#### DonnÃ©es CollectÃ©es

**Pour chaque Ã©quipe :**
- Position au classement
- Nom de l'Ã©quipe
- Nombre de matchs jouÃ©s
- Victoires / Nuls / DÃ©faites
- Buts pour / Buts contre
- DiffÃ©rence de buts
- Points
- Date de scraping

**Statistiques globales :**
- Saison en cours
- JournÃ©e actuelle
- Nombre total d'Ã©quipes
- Date de collecte

#### Pipeline de Traitement

1. **Scraping** : Le spider parcourt la page Wikipedia
2. **Parsing** : Extraction des donnÃ©es du tableau de classement
3. **Validation** : VÃ©rification de la cohÃ©rence des donnÃ©es
4. **Stockage** : Insertion dans MongoDB via le pipeline
5. **Mise Ã  jour** : Upsert basÃ© sur la saison et l'Ã©quipe

#### Scheduler

Le fichier `scheduler.py` utilise la bibliothÃ¨que `schedule` pour :
- Lancer le spider immÃ©diatement au dÃ©marrage (si `--immediate`)
- Planifier des exÃ©cutions pÃ©riodiques (toutes les N heures)
- Logger toutes les opÃ©rations
- GÃ©rer les erreurs et timeouts (max 5 minutes par scraping)

### 2. Base de DonnÃ©es MongoDB

#### Collections

**`ligue1_teams`** : DonnÃ©es des Ã©quipes
```javascript
{
  "saison": "2025-2026",
  "equipe": "Paris Saint-Germain",
  "position": 1,
  "matchs_joues": 20,
  "victoires": 15,
  "nuls": 3,
  "defaites": 2,
  "buts_pour": 45,
  "buts_contre": 18,
  "difference_buts": 27,
  "points": 48,
  "scraped_date": ISODate("2026-02-13T10:30:00Z")
}
```

**`ligue1_stats`** : Statistiques globales
```javascript
{
  "saison": "2025-2026",
  "journee": 20,
  "total_equipes": 18,
  "total_matchs": 180,
  "scraped_date": ISODate("2026-02-13T10:30:00Z")
}
```

#### Indexation
- Index unique sur `{saison, equipe}` pour Ã©viter les doublons
- Index sur `scraped_date` pour les requÃªtes temporelles

### 3. Dashboard Web (Webapp)

#### Technologies
- **Framework** : Dash (Python web framework)
- **Graphiques** : Plotly Express et Graph Objects
- **Styling** : CSS custom avec thÃ¨me sombre

#### Composants Visuels

1. **Header** : Titre et informations de saison
2. **MÃ©triques clÃ©s** : 
   - Nombre d'Ã©quipes
   - Total de matchs
   - DerniÃ¨re mise Ã  jour
3. **Graphiques** :
   - Classement gÃ©nÃ©ral (bar chart)
   - Distribution des points (histogram)
   - Buts pour vs Buts contre (scatter plot)
   - Ã‰volution des performances (line chart)
4. **Tableau dÃ©taillÃ©** : Liste complÃ¨te avec toutes les statistiques
5. **Auto-refresh** : Mise Ã  jour toutes les 5 minutes

#### Palettes de Couleurs
```python
COLORS = {
    'background': '#0e1117',   # Fond sombre
    'card': '#1e2130',         # Cartes
    'text': '#ffffff',         # Texte blanc
    'primary': '#00d4ff',      # Bleu primaire
    'secondary': '#ff4b4b',    # Rouge secondaire
    'success': '#00ff88',      # Vert succÃ¨s
    'warning': '#ffaa00',      # Orange warning
}
```

---

## ğŸ› ï¸ Commandes Utiles

### Gestion des Conteneurs

```bash
# DÃ©marrer tous les services
docker-compose up -d

# ArrÃªter tous les services
docker-compose down

# RedÃ©marrer un service spÃ©cifique
docker-compose restart spider
docker-compose restart webapp
docker-compose restart mongodb

# Voir les logs en temps rÃ©el
docker-compose logs -f spider
docker-compose logs -f webapp --tail 50

# Reconstruire les images
docker-compose build
docker-compose build --no-cache  # Sans cache
```

### AccÃ¨s aux Conteneurs

```bash
# Shell dans le conteneur spider
docker-compose exec spider /bin/bash

# Shell dans le conteneur webapp
docker-compose exec webapp /bin/bash

# AccÃ¨s MongoDB
docker-compose exec mongodb mongosh -u admin -p password123
```

### MongoDB - RequÃªtes Manuelles

```bash
# Connexion Ã  MongoDB
docker-compose exec mongodb mongosh -u admin -p password123 --authenticationDatabase admin

# Dans le shell MongoDB :
use ligue1_db

# Compter les Ã©quipes
db.ligue1_teams.countDocuments()

# Voir le classement
db.ligue1_teams.find().sort({position: 1}).limit(5)

# Voir les derniÃ¨res donnÃ©es
db.ligue1_teams.find().sort({scraped_date: -1}).limit(1)

# Supprimer toutes les donnÃ©es
db.ligue1_teams.deleteMany({})
db.ligue1_stats.deleteMany({})
```

### DÃ©bogage

```bash
# VÃ©rifier la santÃ© des services
python health_check.py

# Lancer manuellement le spider (dans le conteneur)
docker-compose exec spider scrapy crawl ligue1

# Tester la connexion MongoDB
docker-compose exec spider python -c "from pymongo import MongoClient; print(MongoClient('mongodb://admin:password123@mongodb:27017/').server_info())"
```

---

## ğŸ¯ Pourquoi on a choisi ces technos ?

### 1. Scrapy pour le Web Scraping

**Pourquoi c'est cool :**
- C'est un vieux de la vieille (dans le bon sens), super stable et fiable
- Parsing HTML facile comme bonjour avec les CSS Selectors
- Un systÃ¨me de pipeline extensible (vous pouvez y brancher ce que vous voulez)
- Gestion d'erreurs automatique (il rÃ©essaye tout seul comme un grand)
- Ultra rapide grÃ¢ce Ã  l'asynchrone
- Un tas de middleware pour personnaliser

**On a aussi pensÃ© Ã  :** BeautifulSoup + Requests, mais franchement, moins puissant et pas de pipeline intÃ©grÃ©

### 2. MongoDB comme Base de DonnÃ©es

**Pourquoi MongoDB et pas une DB classique :**
- Pas de schÃ©ma strict = libertÃ© totale ! (parfait quand on scrappe et qu'on ne sait pas trop Ã  quoi s'attendre)
- Les agrÃ©gations sont super rapides (important pour nos stats)
- L'upsert intÃ©grÃ© Ã©vite les doublons sans se prendre la tÃªte
- Vous voulez scaler ? Facile, vous ajoutez des serveurs
- IdÃ©al pour nos donnÃ©es un peu "freestyle" de scraping

**L'alternative :** PostgreSQL, mais trop rigide pour notre cas (faudrait tout dÃ©finir Ã  l'avance, bof...)

### 3. Dash/Plotly pour le Dashboard

**Pourquoi on adore Dash :**
- Tout en Python ! Pas besoin de jongler avec du JavaScript (ouf ğŸ˜…)
- Des graphiques interactifs magnifiques sans transpirer
- Les callbacks sont super intuitifs (mÃªme un lundi matin)
- On prototype en 2h ce qui prendrait une journÃ©e ailleurs
- Responsive et moderne par dÃ©faut

**On aurait pu faire :** Flask + Chart.js, mais Ã§a demande plus de code frontend et on aime pas se compliquer la vie

### 4. Docker Compose pour l'Orchestration

**Docker Compose, c'est la vie :**
- "Ã‡a marche sur ma machine" ? Avec Docker, Ã§a marche partout ! ğŸ‰
- Chaque service dans sa bulle (ils peuvent pas se marcher dessus)
- Plus de "pip install" Ã  n'en plus finir, tout est automatisÃ©
- Les conteneurs se parlent entre eux tout seuls (comme des grands)
- Vos donnÃ©es persistent mÃªme si vous redÃ©marrez tout
- Les health checks vous prÃ©viennent si un truc dÃ©conne

**Sans Docker ?** Bon courage pour tout installer Ã  la main... on vous souhaite bien du plaisir ğŸ˜¬

### 5. Schedule pour la Planification

**Simple mais efficace :**
- LÃ©ger comme une plume
- Une API tellement claire qu'on comprend en 2 secondes
- Pas besoin de se battre avec cron ou systemd
- Compatible partout (Windows, Linux, Mac... partout !)
- Parfait pour nos besoins simples

**On aurait pu faire :** Celery + Redis, mais c'est un peu comme prendre un marteau-piqueur pour planter un clou

---

## ğŸ” Ã‡a marche pas ? On vous aide !

### Les conteneurs veulent pas dÃ©marrer ğŸ˜¤

```bash
# VÃ©rifier les ports occupÃ©s
netstat -ano | findstr :27017  # MongoDB
netstat -ano | findstr :8050   # Dashboard

# VÃ©rifier les logs
docker-compose logs

# Nettoyer et redÃ©marrer
docker-compose down -v
docker-compose up -d --force-recreate
```

### Le spider fait la grÃ¨ve ğŸ•·ï¸

```bash
# VÃ©rifier les logs du spider
docker-compose logs spider --tail 100

# Lancer manuellement
docker-compose exec spider scrapy crawl ligue1

# VÃ©rifier la connexion MongoDB depuis le spider
docker-compose exec spider python -c "
from pymongo import MongoClient
client = MongoClient('mongodb://admin:password123@mongodb:27017/')
print(client.server_info())
"
```

### ProblÃ¨me : Le dashboard affiche "No data"

```bash
# VÃ©rifier si des donnÃ©es existent dans MongoDB
docker-compose exec mongodb mongosh -u admin -p password123 --eval "
  use ligue1_db;
  db.ligue1_teams.countDocuments();
"

# Si vide, lancer un scraping manuel
docker-compose exec spider scrapy crawl ligue1
```

### MongoDB joue Ã  cache-cache ğŸ™ˆ

```bash
# VÃ©rifier le health check
docker-compose ps

# RedÃ©marrer MongoDB
docker-compose restart mongodb

# VÃ©rifier les credentials
docker-compose exec mongodb mongosh -u admin -p password123 --authenticationDatabase admin
```

---

## ğŸ“ˆ Ce qu'on aimerait ajouter (un jour... peut-Ãªtre)

### Dans les prochaines semaines (si on a le temps)
- [ ] Des tests (oui, on sait, on devrait...)
- [ ] Notifications Discord/Slack quand votre Ã©quipe gagne (ou perd ğŸ˜¢)
- [ ] Graphiques historiques pour voir l'Ã©volution sur la saison
- [ ] Une vraie API REST (FastAPI, parce que c'est classe)
- [ ] Calendrier des matchs Ã  venir

### Dans quelques mois (si on est motivÃ©s)
- [ ] Scraper d'autres sources (L'Ã‰quipe, site officiel de la LFP...)
- [ ] Stats des joueurs individuels (MbappÃ© vs Haaland, let's go!)
- [ ] Un cache Redis pour aller encore plus vite
- [ ] Exports PDF/Excel pour impressionner vos collÃ¨gues
- [ ] Un systÃ¨me de login (parce que c'est la classe)

### Dans nos rÃªves les plus fous ğŸ’­
- [ ] Une IA pour prÃ©dire les matchs (on sera riches !)
- [ ] Passer sur Kubernetes (pour faire les pros)
- [ ] Une app mobile parce que c'est 2026 quand mÃªme
- [ ] Support de toutes les ligues du monde (bon, au moins la Premier League)
- [ ] SystÃ¨me de pronos pour faire mumuse avec les potes

---

## ğŸ“ Maintenance

### Mise Ã  jour des DÃ©pendances

```bash
# Dans scraper/
pip install --upgrade scrapy pymongo

# Dans webapp/
pip install --upgrade dash plotly pandas

# Mettre Ã  jour les requirements.txt
pip freeze > requirements.txt
```

### Sauvegarde MongoDB

```bash
# Backup
docker-compose exec mongodb mongodump --username admin --password password123 --authenticationDatabase admin --out /data/backup

# Restore
docker-compose exec mongodb mongorestore --username admin --password password123 --authenticationDatabase admin /data/backup
```

---

## ğŸ‘¥ Envie de contribuer ?

On adore les contributions ! Si vous avez une idÃ©e ou si vous voulez corriger un truc :

1. Forkez le projet (c'est pas douloureux, promis)
2. CrÃ©ez votre branche (`git checkout -b feature/MonIdeeDeFou`)
3. Commitez vos changements (`git commit -m 'Ajout de la fonctionnalitÃ© qui dÃ©chire'`)
4. Poussez tout Ã§a (`git push origin feature/MonIdeeDeFou`)
5. Ouvrez une Pull Request et on discute ! â˜•

Pas besoin d'Ãªtre un expert, tout le monde est le bienvenu ! ğŸ¤—

---

## ğŸ“„ Licence

Ce projet est sous licence MIT. Voir le fichier `LICENSE` pour plus de dÃ©tails.

---

## ğŸ™ Remerciements

- **Wikipedia** pour les donnÃ©es publiques
- **Scrapy Project** pour le framework de scraping
- **Plotly/Dash** pour les outils de visualisation
- **MongoDB** pour la base de donnÃ©es

---

## ğŸ“ Contact

Pour toute question ou suggestion :
- **GitHub** : 
[Antoine Chen](https://github.com/Sachachen) et
[Adam Nouari](https://github.com/adam-nouari)
- **Repository** : [DataEngineering](https://github.com/Sachachen/DataEngineering)

---

**Fait avec â¤ï¸, â˜• et beaucoup de passion pour le foot franÃ§ais**

*PS : Si ce projet vous a aidÃ© ou vous a fait gagner du temps, n'hÃ©sitez pas Ã  lui mettre une petite â­ sur GitHub, Ã§a fait toujours plaisir !*

