# ‚öΩ Ligue 1 Analytics - Plateforme de Scraping et Visualisation

## üìã Description du Projet

Plateforme compl√®te d'analyse et de visualisation des donn√©es du championnat de France de football (Ligue 1). Le projet combine un syst√®me de scraping automatis√© avec un dashboard interactif pour suivre en temps r√©el le classement et les statistiques des √©quipes.

### Fonctionnalit√©s principales

- üï∑Ô∏è **Scraping automatis√©** : Collecte p√©riodique des donn√©es depuis Wikipedia
- üìä **Dashboard interactif** : Visualisation en temps r√©el via Dash/Plotly
- üíæ **Stockage MongoDB** : Base de donn√©es NoSQL pour la persistance
- üê≥ **Architecture containeris√©e** : D√©ploiement facile avec Docker Compose
- üîÑ **Mise √† jour automatique** : Scheduler configurable pour les scraping p√©riodiques
- üìà **Analytics avanc√©s** : Graphiques, tableaux et statistiques d√©taill√©es

---

## üèóÔ∏è Architecture Technique

### Stack Technologique

| Composant | Technologie | Version | R√¥le |
|-----------|------------|---------|------|
| **Web Scraping** | Scrapy | 2.11.0 | Framework de scraping |
| **Scheduler** | Schedule | 1.2.0 | Planification des t√¢ches |
| **Base de donn√©es** | MongoDB | 7.0 | Stockage NoSQL |
| **Backend** | Python | 3.x | Logique m√©tier |
| **Frontend** | Dash | 2.14.2 | Framework web interactif |
| **Visualisation** | Plotly | 5.18.0 | Graphiques interactifs |
| **Containerisation** | Docker | - | Orchestration des services |
| **ODM** | PyMongo | 4.6.1 | Driver MongoDB |

### Architecture des Services

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                   Docker Compose                         ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ   Spider        ‚îÇ    MongoDB      ‚îÇ    Dashboard        ‚îÇ
‚îÇ   Container     ‚îÇ    Container    ‚îÇ    Container        ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ - Scrapy        ‚îÇ - Mongo 7.0     ‚îÇ - Dash/Plotly       ‚îÇ
‚îÇ - Scheduler     ‚îÇ - Port: 27017   ‚îÇ - Port: 8050        ‚îÇ
‚îÇ - Python        ‚îÇ - Volumes       ‚îÇ - MongoDB Client    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
        ‚îÇ                  ‚îÇ                  ‚îÇ
        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                           ‚îÇ
                   ligue1-network
```

### Structure du Projet

```
DataEngineering/
‚îú‚îÄ‚îÄ docker-compose.yml          # Orchestration des services
‚îú‚îÄ‚îÄ health_check.py             # Script de v√©rification des services
‚îÇ
‚îú‚îÄ‚îÄ scraper/                    # Service de scraping
‚îÇ   ‚îú‚îÄ‚îÄ Dockerfile              # Image Docker du spider
‚îÇ   ‚îú‚îÄ‚îÄ requirements.txt        # D√©pendances Python
‚îÇ   ‚îú‚îÄ‚îÄ scheduler.py            # Scheduler de scraping
‚îÇ   ‚îú‚îÄ‚îÄ scrapy.cfg              # Configuration Scrapy
‚îÇ   ‚îî‚îÄ‚îÄ ligue1_scraper/         # Projet Scrapy
‚îÇ       ‚îú‚îÄ‚îÄ __init__.py
‚îÇ       ‚îú‚îÄ‚îÄ items.py            # D√©finition des items
‚îÇ       ‚îú‚îÄ‚îÄ pipelines.py        # Pipeline MongoDB
‚îÇ       ‚îú‚îÄ‚îÄ settings.py         # Configuration Scrapy
‚îÇ       ‚îî‚îÄ‚îÄ spiders/
            ‚îî‚îÄ‚îÄ __init__.py
‚îÇ           ‚îî‚îÄ‚îÄ ligue1_spider.py  # Spider principal
‚îÇ
‚îî‚îÄ‚îÄ webapp/                     # Service dashboard
    ‚îú‚îÄ‚îÄ Dockerfile              # Image Docker du dashboard
    ‚îú‚îÄ‚îÄ requirements.txt        # D√©pendances Python
    ‚îú‚îÄ‚îÄ app.py                  # Application Dash
    ‚îî‚îÄ‚îÄ mongo_client.py         # Client MongoDB
```

---

## üöÄ Installation et Lancement

### Pr√©requis

- **Docker** >= 20.10
- **Docker Compose** >= 2.0
- **Git** (pour cloner le projet)
- **Ports disponibles** : 27017 (MongoDB), 8050 (Dashboard)

### 1. Cloner le Projet

```bash
git clone https://github.com/Sachachen/DataEngineering.git
cd DataEngineering
```

### 2. Configuration de l'Environnement

```bash
# Cr√©er un environnement virtuel Python
python -m venv venv

# Activer l'environnement (Windows)
venv\Scripts\activate

# Activer l'environnement (Linux/Mac)
source venv/bin/activate

### 3. Lancer l'Application

```bash
# D√©marrage de tous les services
docker-compose up -d
```

#### M√©thode d√©taill√©e

```bash
# Construction des images Docker
docker-compose build

# Lancement des services en arri√®re-plan
docker-compose up -d

# Consultation des logs en temps r√©el
docker-compose logs -f
```

### 4. V√©rification du D√©ploiement

```bash
# V√©rifier l'√©tat des conteneurs
docker-compose ps

# V√©rifier les logs de chaque service
docker-compose logs mongodb
docker-compose logs spider
docker-compose logs webapp

# Lancer le health check
python health_check.py
```

### 5. Acc√®s au Dashboard

Le dashboard est accessible via navigateur √† l'adresse : **http://localhost:8050**

---


### Variables d'Environnement

#### MongoDB
- `MONGO_INITDB_ROOT_USERNAME` : admin
- `MONGO_INITDB_ROOT_PASSWORD` : password123
- `MONGO_INITDB_DATABASE` : ligue1_db

#### Spider
- `MONGO_URI` : URI de connexion MongoDB
- `LOG_LEVEL` : Niveau de logs (INFO, DEBUG, WARNING)

#### Webapp
- `MONGO_URI` : URI de connexion MongoDB

### Configuration du Scheduler

Le spider peut √™tre configur√© avec diff√©rents param√®tres :

```bash
# Lancer imm√©diatement puis toutes les heures
docker-compose up -d  # Par d√©faut : --immediate --interval 1

# Modifier l'intervalle de scraping
# Editer docker-compose.yml ligne :
command: python scheduler.py --immediate --interval 2  # Toutes les 2 heures
```

Options du scheduler :
- `--immediate` : Lance un scraping au d√©marrage
- `--interval N` : Interval en heures entre chaque scraping (d√©faut: 1)

---

## üìä Fonctionnement D√©taill√©

### 1. Service de Scraping (Spider)

#### Source de Donn√©es
- URL : `https://fr.wikipedia.org/wiki/Championnat_de_France_de_football_2025-2026`
- Parser : HTML via Scrapy CSS Selectors

#### Donn√©es Collect√©es

**Pour chaque √©quipe :**
- Position au classement
- Nom de l'√©quipe
- Nombre de matchs jou√©s
- Victoires / Nuls / D√©faites
- Buts pour / Buts contre
- Diff√©rence de buts
- Points
- Date de scraping

**Statistiques globales :**
- Saison en cours
- Journ√©e actuelle
- Nombre total d'√©quipes
- Date de collecte

#### Pipeline de Traitement

1. **Scraping** : Le spider parcourt la page Wikipedia
2. **Parsing** : Extraction des donn√©es du tableau de classement
3. **Validation** : V√©rification de la coh√©rence des donn√©es
4. **Stockage** : Insertion dans MongoDB via le pipeline
5. **Mise √† jour** : Upsert bas√© sur la saison et l'√©quipe

#### Scheduler

Le fichier `scheduler.py` utilise la biblioth√®que `schedule` pour :
- Lancer le spider imm√©diatement au d√©marrage (si `--immediate`)
- Planifier des ex√©cutions p√©riodiques (toutes les N heures)
- Logger toutes les op√©rations
- G√©rer les erreurs et timeouts (max 5 minutes par scraping)

### 2. Base de Donn√©es MongoDB

#### Collections

**`ligue1_teams`** : Donn√©es des √©quipes
```javascript
{
  "saison": "2025-2026",
  "equipe": "Paris Saint-Germain",
  "position": 1,
  "matchs_joues": 20,
  "victoires": 15,
  "nuls": 3,
  "defaites": 2,
  "buts_pour": 45,
  "buts_contre": 18,
  "difference_buts": 27,
  "points": 48,
  "scraped_date": ISODate("2026-02-13T10:30:00Z")
}
```

**`ligue1_stats`** : Statistiques globales
```javascript
{
  "saison": "2025-2026",
  "journee": 20,
  "total_equipes": 18,
  "total_matchs": 180,
  "scraped_date": ISODate("2026-02-13T10:30:00Z")
}
```

#### Indexation
- Index unique sur `{saison, equipe}` pour √©viter les doublons
- Index sur `scraped_date` pour les requ√™tes temporelles

### 3. Dashboard Web (Webapp)

#### Technologies
- **Framework** : Dash (Python web framework)
- **Graphiques** : Plotly Express et Graph Objects
- **Styling** : CSS custom avec th√®me sombre

#### Composants Visuels

1. **Header** : Titre et informations de saison
2. **M√©triques cl√©s** : 
   - Nombre d'√©quipes
   - Total de matchs
   - Derni√®re mise √† jour
3. **Graphiques** :
   - Classement g√©n√©ral (bar chart)
   - Distribution des points (histogram)
   - Buts pour vs Buts contre (scatter plot)
   - √âvolution des performances (line chart)
4. **Tableau d√©taill√©** : Liste compl√®te avec toutes les statistiques
5. **Auto-refresh** : Mise √† jour toutes les 5 minutes

#### Palettes de Couleurs
```python
COLORS = {
    'background': '#0e1117',   # Fond sombre
    'card': '#1e2130',         # Cartes
    'text': '#ffffff',         # Texte blanc
    'primary': '#00d4ff',      # Bleu primaire
    'secondary': '#ff4b4b',    # Rouge secondaire
    'success': '#00ff88',      # Vert succ√®s
    'warning': '#ffaa00',      # Orange warning
}
```

---

## üõ†Ô∏è Commandes Utiles

### Gestion des Conteneurs

```bash
# D√©marrer tous les services
docker-compose up -d

# Arr√™ter tous les services
docker-compose down

# Red√©marrer un service sp√©cifique
docker-compose restart spider
docker-compose restart webapp
docker-compose restart mongodb

# Voir les logs en temps r√©el
docker-compose logs -f spider
docker-compose logs -f webapp --tail 50

# Reconstruire les images
docker-compose build
docker-compose build --no-cache  # Sans cache
```

### Acc√®s aux Conteneurs

```bash
# Shell dans le conteneur spider
docker-compose exec spider /bin/bash

# Shell dans le conteneur webapp
docker-compose exec webapp /bin/bash

# Acc√®s MongoDB
docker-compose exec mongodb mongosh -u admin -p password123
```

### MongoDB - Requ√™tes Manuelles

```bash
# Connexion √† MongoDB
docker-compose exec mongodb mongosh -u admin -p password123 --authenticationDatabase admin

# Dans le shell MongoDB :
use ligue1_db

# Compter les √©quipes
db.ligue1_teams.countDocuments()

# Voir le classement
db.ligue1_teams.find().sort({position: 1}).limit(5)

# Voir les derni√®res donn√©es
db.ligue1_teams.find().sort({scraped_date: -1}).limit(1)

# Supprimer toutes les donn√©es
db.ligue1_teams.deleteMany({})
db.ligue1_stats.deleteMany({})
```

### D√©bogage

```bash
# V√©rifier la sant√© des services
python health_check.py

# Lancer manuellement le spider (dans le conteneur)
docker-compose exec spider scrapy crawl ligue1

# Tester la connexion MongoDB
docker-compose exec spider python -c "from pymongo import MongoClient; print(MongoClient('mongodb://admin:password123@mongodb:27017/').server_info())"
```

---

## üéØ Choix Technologiques

### 1. Scrapy pour le Web Scraping

**Avantages :**
- Parsing HTML optimis√© avec s√©lecteurs CSS
- Syst√®me de pipeline extensible et modulaire
- Gestion automatique des erreurs et retry mechanism
- Performance √©lev√©e gr√¢ce √† l'architecture asynchrone
- Middleware personnalisable pour √©tendre les fonctionnalit√©s

**Alternative consid√©r√©e :** BeautifulSoup + Requests, solution moins performante et sans syst√®me de pipeline int√©gr√©

### 2. MongoDB comme Base de Donn√©es

**Avantages :**
- Sch√©ma flexible adapt√© aux donn√©es de scraping √©volutives
- Performance optimale pour les op√©rations d'agr√©gation
- Op√©ration upsert native pour √©viter les doublons
- Solution id√©ale pour le stockage de donn√©es semi-structur√©es

**Alternative consid√©r√©e :** PostgreSQL, n√©cessitant un sch√©ma rigide moins adapt√© aux donn√©es de scraping

### 3. Dash/Plotly pour le Dashboard

**Avantages de Dash :**
- Framework enti√®rement en Python, √©liminant la n√©cessit√© de d√©velopper en JavaScript
- Biblioth√®que de graphiques interactifs haute performance
- Syst√®me de callbacks r√©actifs intuitif et efficace
- D√©veloppement rapide gr√¢ce √† une API de haut niveau
- Interface responsive et moderne par d√©faut

**Alternative consid√©r√©e :** Flask + Chart.js, n√©cessitant cependant plus de d√©veloppement frontend et une architecture plus complexe

### 4. Docker Compose pour l'Orchestration

**Avantages de Docker Compose :**
- Garantie de reproductibilit√© de l'environnement sur toutes les plateformes
- Isolation des services pour √©viter les conflits de d√©pendances
- Automatisation compl√®te de l'installation des d√©pendances
- R√©seau interne permettant la communication inter-conteneurs
- Persistance des donn√©es via volumes Docker
- Surveillance de l'√©tat des services avec health checks int√©gr√©s

**Alternative :** Installation manuelle n√©cessitant une configuration complexe et une gestion individuelle de chaque d√©pendance

### 5. Schedule pour la Planification

**Avantages de Schedule :**
- Biblioth√®que l√©g√®re avec faible empreinte m√©moire
- API simple et intuitive
- Ind√©pendant des utilitaires syst√®me (cron, systemd)
- Compatibilit√© multiplateforme (Windows, Linux, macOS)
- Solution adapt√©e aux besoins de planification p√©riodique du projet

**Alternative consid√©r√©e :** Celery + Redis, solution plus complexe offrant des fonctionnalit√©s avanc√©es non n√©cessaires pour ce cas d'usage

---

## üîç Guide de D√©pannage

### Probl√®me de d√©marrage des conteneurs

```bash
# V√©rifier les ports occup√©s
netstat -ano | findstr :27017  # MongoDB
netstat -ano | findstr :8050   # Dashboard

# V√©rifier les logs
docker-compose logs

# Nettoyer et red√©marrer
docker-compose down -v
docker-compose up -d --force-recreate
```

### Erreur du service de scraping

```bash
# V√©rifier les logs du spider
docker-compose logs spider --tail 100

# Lancer manuellement
docker-compose exec spider scrapy crawl ligue1

# V√©rifier la connexion MongoDB depuis le spider
docker-compose exec spider python -c "
from pymongo import MongoClient
client = MongoClient('mongodb://admin:password123@mongodb:27017/')
print(client.server_info())
"
```

### Probl√®me : Le dashboard affiche "No data"

```bash
# V√©rifier si des donn√©es existent dans MongoDB
docker-compose exec mongodb mongosh -u admin -p password123 --eval "
  use ligue1_db;
  db.ligue1_teams.countDocuments();
"

# Si vide, lancer un scraping manuel
docker-compose exec spider scrapy crawl ligue1
```

### Probl√®me de connexion MongoDB

```bash
# V√©rifier le health check
docker-compose ps

# Red√©marrer MongoDB
docker-compose restart mongodb

# V√©rifier les credentials
docker-compose exec mongodb mongosh -u admin -p password123 --authenticationDatabase admin
```

---

## üìà Roadmap et √âvolutions Futures

### Court terme
- [ ] Impl√©mentation de tests unitaires et d'int√©gration
- [ ] Syst√®me de notifications (Discord/Slack) pour les √©v√©nements sportifs
- [ ] Graphiques d'√©volution temporelle sur la saison
- [ ] API REST avec FastAPI pour l'acc√®s programmatique aux donn√©es
- [ ] Int√©gration du calendrier des matchs

### Moyen terme
- [ ] Diversification des sources de donn√©es (L'√âquipe, LFP)
- [ ] Extension aux statistiques individuelles des joueurs
- [ ] Impl√©mentation d'un cache Redis pour optimisation des performances
- [ ] Fonctionnalit√©s d'export (PDF, Excel, CSV)
- [ ] Syst√®me d'authentification et de gestion des utilisateurs

### Long terme
- [ ] Mod√®les pr√©dictifs bas√©s sur l'apprentissage automatique
- [ ] Migration vers Kubernetes pour scalabilit√©
- [ ] D√©veloppement d'une application mobile
- [ ] Extension √† d'autres championnats europ√©ens
- [ ] Plateforme de pronostics collaborative

---

## üìù Maintenance

### Mise √† jour des D√©pendances

```bash
# Dans scraper/
pip install --upgrade scrapy pymongo

# Dans webapp/
pip install --upgrade dash plotly pandas

# Mettre √† jour les requirements.txt
pip freeze > requirements.txt
```

### Sauvegarde MongoDB

```bash
# Backup
docker-compose exec mongodb mongodump --username admin --password password123 --authenticationDatabase admin --out /data/backup

# Restore
docker-compose exec mongodb mongorestore --username admin --password password123 --authenticationDatabase admin /data/backup
```

---

## üë• Contribution

Les contributions sont les bienvenues. Pour contribuer au projet :

1. Forkez le repository
2. Cr√©ez votre branche de fonctionnalit√© (`git checkout -b feature/NouvelleFonctionnalite`)
3. Committez vos modifications (`git commit -m 'Ajout d'une nouvelle fonctionnalit√©'`)
4. Pushez vers la branche (`git push origin feature/NouvelleFonctionnalite`)
5. Ouvrez une Pull Request

Toutes les contributions, qu'elles soient mineures ou majeures, sont appr√©ci√©es.

---

## üìÑ Licence

Ce projet est sous licence MIT. Voir le fichier `LICENSE` pour plus de d√©tails.

---

## üôè Remerciements

- **Wikipedia** pour les donn√©es publiques
- **Scrapy Project** pour le framework de scraping
- **Plotly/Dash** pour les outils de visualisation
- **MongoDB** pour la base de donn√©es

---

## üìû Contact

Pour toute question ou suggestion :
- **GitHub** : 
[Antoine Chen](https://github.com/Sachachen) et
[Adam Nouari](https://github.com/adam-nouari)
- **Repository** : [DataEngineering](https://github.com/Sachachen/DataEngineering)

---

**D√©velopp√© avec passion pour l'analyse sportive et les donn√©es du football fran√ßais**

*Note : Si ce projet vous a √©t√© utile, n'h√©sitez pas √† lui attribuer une √©toile ‚≠ê sur GitHub.*

